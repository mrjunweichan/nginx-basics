


## Rate Limit
  
**Update API Gateway**  
```sh
sudo vim /etc/nginx/conf.d/api_gateway.conf
```
  
**/etc/nginx/conf.d/api_gateway.conf**  
```nginx
limit_req_zone $remote_addr zone=perip:1m rate=2r/s;

upstream pet-store {
    zone api_upstreams 64k;
    server 10.1.1.4:80;
}
server {
    listen 443 ssl;
    server_name example.api;
    ssl_certificate        /etc/nginx/ssl/server-cert.pem;
    ssl_certificate_key    /etc/nginx/ssl/server-key.pem;
    ssl_client_certificate /etc/nginx/ssl/ca-cert.pem;
    ssl_verify_client on;
    ssl_verify_depth 2;
    
    limit_req zone=perip nodelay;
    limit_req_status 429;
    
    location /pet-store-api/ {
        proxy_pass http://pet-store/v2/;
        proxy_set_header Host $host;
    }
    location /pet-store-api/store/ {
        auth_jwt "store api" token=$arg_token;
        auth_jwt_key_file /etc/nginx/secrets/api_secret.key;
        proxy_set_header Host $host;
        proxy_set_header API-Client $jwt_claim_sub;
        proxy_pass http://pet-store/v2/store/;
    }
}
```

**Explain**
> **limit_req_zone** – Define shared memory zone  
> 	**$remote_addr** – Use client’s ip as key (for tracking)  
> 	**zone=perip:1m** – Name zone perip and allocate 1MB  
> 	**rate=2r/s** – Allow 2 requests per second  
>  
> **limit_req** – Enable the rate limiting  
> 	**zone=perip** – Use zone defined above  
> 	**nodelay** – Reject immediately if exceeds rate (2r/s)  
>  
> **limit_req_status 429** – Sets HTTP code 429 for rejected requestsv
  
  
---
## Caching
  
**Enter and keep refreshing**  
```shell
http://www.example.com:9000
```
> Notice browser will loop between upstream Apps  
  
**Create Cache Directory**  
```sh
sudo mkdir -p /data/nginx/cache
```
  
**Update Load Balancer**  
```nginx
sudo vim /etc/nginx/conf.d/lb.conf
```
  
**/etc/nginx/conf.d/lb.conf**  
```nginx
proxy_cache_path /data/nginx/cache levels=1:2 keys_zone=upstream_cache:20m inactive=5m max_size=2G;

upstream backend_servers {
    zone backend_server_zone 64k;
    server 127.0.0.1:9001;
    server 127.0.0.1:9002;
    server 127.0.0.1:9003;
}

server {
    listen 9000;
    server_name www.example.com;
    autoindex on;
    
    location / {
        proxy_pass http://backend_servers/;
        
        proxy_cache upstream_cache;
        proxy_cache_key $scheme$host$request_uri;
        proxy_cache_valid 5m;
        add_header X-Cache-Status $upstream_cache_status;
        
        proxy_set_header Host $host;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Real-IP  $remote_addr;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
    }
}
```
  
**Enter and keep refreshing**  
```shell
http://www.example.com:9000
```
> Notice browser will stick to 1 app since request is cached  

  
---
## Active Health Checks ![[NGINX+.png|30]]

Probes your backend applications with request to check if they are up and healthy.  
If health check fails on upstream server, it is marked as **down** and will not be used for **NEW** connections.  
Existing connections/requests are left intact.  
  
  
**Update Load Balancer**  
```sh
sudo vim /etc/nginx/conf.d/lb.conf
```
  
**/etc/nginx/conf.d/lb.conf**  
```nginx
proxy_cache_path /data/nginx/cache levels=1:2 keys_zone=upstream_cache:20m inactive=5m max_size=2G;

upstream backend_servers {
    zone backend_server_zone 64k;
    server 127.0.0.1:9001;
    server 127.0.0.1:9002;
    server 127.0.0.1:9003;
}

server {
    listen 9000;
    autoindex on;
    
    location / {
        proxy_pass http://backend_servers/;
        
        proxy_cache upstream_cache;
        proxy_cache_key $scheme$host$request_uri;
        proxy_cache_valid 5m;
        add_header X-Cache-Status $upstream_cache_status;
        
        health_check interval=5s fails=3 passes=2;
        
        proxy_set_header Host $host;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Real-IP  $remote_addr;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
	}
}
```
> `internal` makes location only accessible **internally** so only NGINX can issue request to this block  
> `proxy_pass` defines which upstream to check  
> `health_check`  
> 	`interval`defines how often to check  
> 	`fails`mark server unhealthy after X failures  
> 	`passes` marks server healthy after X success  
> 	`uri` defines the request uri NGINX uses for the check  
> 	`match` custom response defined in the status_ok.conf file  
  
**Bring Down App1**  
```shell
mv /opt/services/App1 /opt/services/App1a
```
  
**Check NGINX Dashboard**  
  
**Bring up App1**  
```shell
mv /opt/services/App1a /opt/services/App1
```
  
**Check NGINX Dashboard**  
  
**Update API Gateway**  
```sh
sudo vim /etc/nginx/conf.d/api_gateway.conf
```
  
**/etc/nginx/conf.d/api_gateway.conf**
```nginx
upstream pet-store {
    zone api_upstreams 64k;
    server 10.1.1.10:80;
}

server {
    listen 443 ssl;
    server_name example.api;
    ssl_certificate        /etc/nginx/ssl/server-cert.pem;
    ssl_certificate_key    /etc/nginx/ssl/server-key.pem;
    ssl_client_certificate /etc/nginx/ssl/ca-cert.pem;
    ssl_verify_client on;
    ssl_verify_depth 2;
        
    location /pet-store-api/ {
        proxy_pass http://pet-store/v2/;
        proxy_set_header Host $host;
    }
    
    location /pet-store-api/store/ {
        auth_jwt "store api" token=$arg_token;
        auth_jwt_key_file /etc/nginx/secrets/api_secret.key;
        
        proxy_set_header API-Client $jwt_claim_sub;
        
        proxy_cache upstream_cache;
        proxy_cache_key $scheme$host$request_uri;
        proxy_cache_valid 5m;
        add_header X-Cache-Status $upstream_cache_status;
        
        health_check interval=5s fails=3 passes=2;
        
        proxy_set_header Host $host;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Real-IP  $remote_addr;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        
        
        proxy_pass http://pet-store/v2/store/;
    }
}
```

**Note**
> NGINX also records health check failures in `/var/log/nginx/error.log` file  
  
**Print NGINX Error Logs**  
```shell
tail /var/log/nginx/error.log
```
  
**Expected Error #110**  
```txt
2025/06/24 06:54:05 [error] 22#22: upstream timed out (110: Connection timed out) while connecting to upstream, health check "status_ok" of peer ip_address:port in upstream "server_name"
```
**110: Connection timeout (TCP)**  
> NGINX can't establish TCP connection to upstream  
> Common cause:   
> 	- overloaded/hung  
> 	- network latency  
> 	- firewall delay  
> 	- incorrect `proxy_connection_timeout`  
  
**Expected Error #113**  
```txt
2025/06/24 06:54:13 [error] 22#22: connect() failed (113: No route to host) while connecting to upstream (status 0), health check "" of peer 10.1.1.10:80 in upstream "pet-store"
```
**113: No Route to Host (Network)**  
> NGINX can't find route to upstream server's IP  
> Common cause:  
> 	- upstream offline  
> 	- no network connectivity  
> 	- incorrect IP/DNS resolution  
> 	- Firewall blocking traffic  

**Best Practice**  
> When troubleshooting upstream and you have health-checks enabled  
> The first thing you should look for is **#110** and **#113**  
> Immediately indicates that NGINX can't reach upstream due to network config or connectivity issues  


---
**Reference:** 
https://nginx.org/en/docs/http/ngx_http_limit_req_module.html



